{"cells":[{"cell_type":"markdown","metadata":{"id":"vGwaJ0eGHCkw"},"source":["# LoRA Easy Training Colab\n","[![Ko-Fi](https://img.shields.io/badge/Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat)](https://ko-fi.com/jelosus1)\n","\n","### Colab powered by [Lora_Easy_Training_Scripts_Backend](https://github.com/derrian-distro/LoRA_Easy_Training_scripts_Backend/)\n","\n","<h4><font color=\"red\"><u>DISCLAIMER:</u></font> The Forked version of the trainer is not maintained by its original author, please do not open issues there if you encounter any problem. Instead submit them in the <a href=\"https://github.com/Jelosus2/LoRA_Easy_Training_scripts_Backend/\">forked repo</a>.</h4>\n","\n","\n","---\n","\n","\n","Learn how to use the colab [here](https://civitai.com/articles/4409).\n","\n","If you feel something is missing, want something to be added or simply found a bug, open an [issue](https://github.com/Jelosus2/Lora_Easy_Training_Colab/issues).\n","\n","---\n","\n","Last Update: June 30, 2024. Check the [full changelog](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#changelog)\n","\n","Changes:\n","- Fixed wd taggers and BLIP captioning + now all taggers run on onnx run time. Keras run time has been removed since it's actually much slower.\n","- Now you can use paths to specify where you want to setup the LoRA folder instead of just a name on the root of drive/google colab.\n","- Added the ability to give different file names for downloaded models and VAEs. Additionally, now you can save them on your own google drive.\n","- Fixed a bunch of bugs and errors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CSz_rmldHZvh","cellView":"form"},"outputs":[],"source":["# @title ## 1. Install the trainer\n","import os\n","from pathlib import Path\n","\n","root_path = Path(\"/content\")\n","trainer_dir = root_path.joinpath(\"trainer\")\n","\n","venv_pip = trainer_dir.joinpath(\"sd_scripts/venv/bin/pip\")\n","venv_python = trainer_dir.joinpath(\"sd_scripts/venv/bin/python\")\n","\n","# @markdown Select the version of the trainer you want to use. Forked version is an unofficial version that adds some features like different optimizers, schedulers, etc. You can check the features that are available on it [here](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#additions-of-the-forked-version)\n","trainer_version = \"Original\" # @param [\"Original\", \"Forked (Additional features)\"]\n","\n","installed_dependencies = False\n","first_step_done = False\n","\n","def install_trainer():\n","  global installed_dependencies, first_step_done\n","\n","  print(\"Installing trainer...\")\n","  !apt -y update -qq\n","  !apt install -y python3.10-venv aria2 -qq\n","\n","  installed_dependencies = True\n","\n","  if trainer_version == \"Original\":\n","    !git clone https://github.com/derrian-distro/LoRA_Easy_Training_scripts_Backend {trainer_dir}\n","  else:\n","    !git clone https://github.com/Jelosus2/LoRA_Easy_Training_scripts_Backend {trainer_dir}\n","\n","  !chmod 755 /content/trainer/colab_install.sh\n","  os.chdir(trainer_dir)\n","  !./colab_install.sh\n","\n","  if \"Forked\" in trainer_version:\n","    # No use right now\n","    print(\"Patching trainer...\")\n","\n","  os.chdir(root_path)\n","\n","  first_step_done = True\n","  print(\"Installation complete!\")\n","\n","def download_custom_wd_tagger():\n","  global wd_path\n","\n","  wd_path = trainer_dir.joinpath(\"sd_scripts/finetune/tag_images_by_wd14_tagger.py\")\n","\n","  print(\"Downloading tagger script that allows v3 taggers...\")\n","  !rm \"{wd_path}\"\n","  !aria2c \"https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/main/custom/tag_images_by_wd14_tagger.py\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{wd_path}\"\n","\n","def fix_scripts_logging():\n","  print(\"Fixing sd_scripts logging issue on colab...\")\n","  !yes | {venv_pip} uninstall rich\n","\n","def main():\n","  install_trainer()\n","  download_custom_wd_tagger()\n","  fix_scripts_logging()\n","  print(\"Finished installation!\")\n","\n","try:\n","  main()\n","except Exception as e:\n","  print(f\"Error intalling the trainer!\\n{e}\")\n","  first_step_done = False"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"oS4dJqXoiyC5"},"outputs":[],"source":["# @title ## 2. Setup the directories\n","from pathlib import Path\n","from google.colab import drive\n","\n","if not globals().get(\"first_step_done\"):\n","  root_path = Path(\"/content\")\n","  trainer_dir = root_path.joinpath(\"trainer\")\n","\n","drive_dir = root_path.joinpath(\"drive/MyDrive\")\n","pretrained_model_dir = root_path.joinpath(\"pretrained_model\")\n","vae_dir = root_path.joinpath(\"vae\")\n","tagger_models_dir = root_path.joinpath(\"tagger_models\")\n","\n","# @markdown The base path for your project. Make sure it can be used as a folder name\n","project_path = \"Loras/My_first_lora\" # @param {type: \"string\"}\n","# @markdown Specify the name for the directories. If you have multiple datasets, separate each with a comma `(,)` like this: **dataset1, dataset2, ...**\n","\n","# @markdown The directory where the results of the training will be stored.\n","output_dir_name = \"output\" # @param {type: \"string\"}\n","# @markdown The directory where your dataset(s) will be located.\n","dataset_dir_name = \"dataset\" # @param {type: \"string\"}\n","# @markdown Use Drive to store all the files and directories\n","use_drive = True # @param {type: \"boolean\"}\n","\n","project_path = project_path.replace(\" \", \"_\")\n","output_dir_name = output_dir_name.replace(\" \", \"_\")\n","\n","second_step_done = False\n","\n","def is_valid_folder_name(folder_name: str) -> bool:\n","  invalid_characters = '<>:\"/\\|?*'\n","\n","  if any(char in invalid_characters for char in folder_name):\n","    return False\n","\n","  return True\n","\n","def mount_drive_dir() -> Path:\n","  base_dir = root_path.joinpath(project_path)\n","\n","  if use_drive:\n","    if not Path(drive_dir).exists():\n","      drive.mount(Path(drive_dir).parent.as_posix())\n","    base_dir = drive_dir.joinpath(project_path)\n","\n","  return base_dir\n","\n","def make_directories():\n","  mount_drive = mount_drive_dir()\n","  output_dir = mount_drive.joinpath(output_dir_name)\n","\n","  if not Path(mount_drive).exists():\n","    Path(mount_drive).mkdir(exist_ok=True)\n","\n","  for dir in [pretrained_model_dir, vae_dir, output_dir, tagger_models_dir]:\n","    Path(dir).mkdir(exist_ok=True)\n","\n","  for dataset_m_dir in dataset_dir_name.replace(\" \", \"\").split(','):\n","    if is_valid_folder_name(dataset_m_dir):\n","      Path(mount_drive.joinpath(dataset_m_dir)).mkdir(exist_ok=True)\n","    else:\n","      print(f\"{dataset_m_dir} is not a valid name for a folder\")\n","      return\n","\n","def main():\n","  for name in [project_path, output_dir_name]:\n","      if not is_valid_folder_name(name.replace(\"/\", \"\") if project_path == name else name):\n","        print(f\"{name} is not a valid name for a folder\")\n","        return\n","\n","  print(\"Setting up directories...\")\n","  make_directories()\n","  print(\"Done!\")\n","\n","try:\n","  main()\n","  second_step_done = True\n","except Exception as e:\n","  print(f\"Error setting up the directories!\\n{e}\")\n","  second_step_done = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b0_HNDa7Zdei","cellView":"form"},"outputs":[],"source":["# @title ## 3. Download the base model and/or VAE used for training\n","import re\n","from pathlib import Path\n","\n","model_url = \"\"\n","vae_url = \"\"\n","\n","# @markdown Default models are provided here for training. If you want to use another one, introduce the URL in the input below. The link must be pointing to either Civitai or Hugging Face and have the correct format. You can check how to get the correct link [here](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-get-the-link-for-custom-modelvae).\n","training_model = \"(XL) PonyDiffusion v6\" # @param [\"(XL) PonyDiffusion v6\", \"(XL) Animagine 3.1\", \"(XL) SDXL 1.0\", \"(1.5) anime-full-final-pruned (Most used on Anime LoRAs)\", \"(1.5) AnyLora\", \"(1.5) SD 1.5\"]\n","custom_training_model = \"\" # @param {type: \"string\"}\n","# @markdown The name you want to give to the downloaded model file, if not specified default ones will be used.\n","model_name = \"\" # @param {type: \"string\"}\n","# @markdown VAE used for training. It's not needed for 1.5 nor XL, but it's recommended to use the SDXL base VAE for XL training. If you want to use a custom one, introduce the URL in the input below.\n","vae = \"SDXL VAE\" # @param [\"SDXL VAE\", \"None\"]\n","custom_vae = \"\" # @param {type: \"string\"}\n","# @markdown The name you want to give to the downloaded VAE file, if not specified default ones will be used.\n","vae_name = \"\" # @param {type: \"string\"}\n","# @markdown Introduce your [Civitai API Token](https://civitai.com/user/account) or [HuggingFace Access Token](https://huggingface.co/settings/tokens) if the authentication fails while downloading the model and/or VAE.\n","api_token = \"\" # @param {type: \"string\"}\n","# @markdown You can optionally download the model and/or VAE on your drive so you don't need to download them again in the next session. You only would need to specify their path on the UI for the next time you want to use them without downloading.\n","download_in_drive = False # @param {type: \"boolean\"}\n","\n","thrid_step_done = False\n","\n","if custom_training_model:\n","  model_url = custom_training_model\n","elif \"Pony\" in training_model:\n","  model_url = \"https://huggingface.co/AstraliteHeart/pony-diffusion-v6/resolve/main/v6.safetensors\"\n","elif \"Animagine\" in training_model:\n","  model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-3.1/resolve/main/animagine-xl-3.1.safetensors\"\n","elif \"SDXL\" in training_model:\n","  model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\"\n","elif \"anime\" in training_model:\n","  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/animefull-final-pruned-fp16.safetensors\"\n","elif \"Any\" in training_model:\n","  model_url = \"https://huggingface.co/Lykon/AnyLoRA/resolve/main/AnyLoRA_noVae_fp16-pruned.safetensors\"\n","elif \"SD 1.5\" in training_model:\n","  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/sd-v1-5-pruned-noema-fp16.safetensors\"\n","\n","if custom_vae:\n","  vae_url = custom_vae\n","elif \"SDXL\" in vae:\n","  vae_url = \"https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors\"\n","\n","model_file = \"\"\n","vae_file = \"\"\n","\n","header = \"\"\n","\n","if not \"installed_dependencies\" in globals():\n","  print(\"Installing missing dependency...\")\n","  !apt -y update -qq\n","  !apt install -y aria2 -qq\n","  globals().setdefault(\"installed_dependencies\", True)\n","\n","def download_model():\n","  global model_file, model_url, pretrained_model_dir\n","\n","  if re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", model_url):\n","    model_url = model_url.replace(\"blob\", \"resolve\")\n","  elif re.search(r\"https:\\/\\/civitai\\.com\\/models\\/\\d+\", model_url):\n","    if m := re.search(r\"modelVersionId=(\\d+)\", model_url):\n","      model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n","  elif not re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", model_url) and not re.search(r\"https:\\/\\/civitai\\.com\\/api\\/download\\/models\\/(\\d+)\", model_url):\n","    print(\"Invalid model download URL!\\nCheck how to get the correct link in https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-get-the-link-for-custom-modelvae\")\n","    return\n","\n","  if \"civitai.com\" in model_url and api_token and not \"hf\" in api_token:\n","    model_url = f\"{model_url}&token={api_token}\" if \"?\" in model_url else f\"{model_url}?token={api_token}\"\n","  elif \"huggingface.co\" in model_url and api_token:\n","    header = f\"Authorization: Bearer {api_token}\"\n","\n","  stripped_model_url = model_url.strip()\n","\n","  if download_in_drive:\n","    pretrained_model_dir = Path(drive_dir).joinpath(\"Downloaded_models\")\n","\n","    if not Path(pretrained_model_dir).exists():\n","      Path(pretrained_model_dir).mkdir(exist_ok=True)\n","\n","  if model_name:\n","    validated_name = model_name.translate(str.maketrans('', '', '\\\\/:*?\"<>|'))\n","\n","    if not validated_name.endswith((\".ckpt\", \".safetensors\")):\n","      model_file = pretrained_model_dir.joinpath(f\"{validated_name}.safetensors\")\n","    else:\n","      model_file = pretrained_model_dir.joinpath(validated_name)\n","  elif stripped_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n","    model_file = pretrained_model_dir.joinpath(stripped_model_url[stripped_model_url.rfind('/'):].replace(\"/\", \"\"))\n","  else:\n","    model_file = pretrained_model_dir.joinpath(\"downloaded_model.safetensors\")\n","    if Path(model_file).exists() and not download_in_drive:\n","      !rm \"{model_file}\"\n","\n","  print(f\"Downloading model from {model_url}...\")\n","  !aria2c \"{model_url}\" --console-log-level=warn --header=\"{header}\" -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n","\n","def download_vae():\n","  global vae_file, vae_url, vae_dir\n","\n","  if not vae == \"None\":\n","    if re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", vae_url):\n","      vae_url = vae_url.replace(\"blob\", \"resolve\")\n","    elif re.search(r\"https:\\/\\/civitai\\.com\\/models\\/\\d+\", vae_url):\n","      if m := re.search(r\"modelVersionId=(\\d+)\", vae_url):\n","        vae_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n","    elif not re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", vae_url) and not re.search(r\"https:\\/\\/civitai\\.com\\/api\\/download\\/models\\/(\\d+)\", vae_url):\n","      print(\"Invalid VAE download URL!\\nCheck how to get the correct link in https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-get-the-link-for-custom-modelvae\")\n","      return\n","\n","    if \"civitai.com\" in vae_url and api_token and not \"hf\" in api_token:\n","      vae_url = f\"{vae_url}&token={api_token}\" if \"?\" in vae_url else f\"{vae_url}?token={api_token}\"\n","    elif \"huggingface.co\" in vae_url and api_token:\n","      header = f\"Authorization: Bearer {api_token}\"\n","\n","    stripped_model_vae = vae_url.strip()\n","\n","    if download_in_drive:\n","      vae_dir = Path(drive_dir).joinpath(\"Downloaded_VAEs\")\n","\n","      if not Path(vae_dir).exists():\n","        Path(vae_dir).mkdir(exist_ok=True)\n","\n","    if vae_name:\n","      validated_name = vae_name.translate(str.maketrans('', '', '\\\\/:*?\"<>|'))\n","\n","      if not validated_name.endswith((\".ckpt\", \".safetensors\")):\n","        vae_file = vae_dir.joinpath(f\"{validated_name}.safetensors\")\n","      else:\n","        vae_file = vae_dir.joinpath(validated_name)\n","    elif stripped_model_vae.lower().endswith((\".ckpt\", \".safetensors\")):\n","      vae_file = vae_dir.joinpath(stripped_model_vae[stripped_model_vae.rfind('/'):].replace(\"/\", \"\"))\n","    else:\n","      vae_file = vae_dir.joinpath(\"downloaded_vae.safetensors\")\n","      if Path(vae_file).exists() and not download_in_drive:\n","        !rm \"{vae_file}\"\n","\n","    print(f\"Downloading vae from {vae_url}...\")\n","    !aria2c \"{vae_url}\" --console-log-level=warn --header=\"{header}\" -c -s 16 -x 16 -k 10M -d / -o \"{vae_file}\"\n","  else:\n","    vae_file = \"\"\n","\n","def main():\n","  if not globals().get(\"second_step_done\"):\n","    print(\"You have to run the 2nd step first!\")\n","    return\n","\n","  if download_in_drive and not use_drive:\n","    print(\"You are trying to download the model and/or VAE in your drive but you didn't mount it. Please select the 'use_drive' option in 2nd step.\")\n","    return\n","\n","  download_model()\n","  download_vae()\n","\n","try:\n","  main()\n","  thrid_step_done = True\n","except Exception as e:\n","  print(f\"Failed to download the models\\n{e}\")\n","  thrid_step_done = False"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"66XBK6B_iSYj"},"outputs":[],"source":["# @title ## 4. Upload your dataset\n","import re\n","import zipfile\n","from pathlib import Path\n","\n","# @markdown ### Unzip the dataset\n","# @markdown If you have a dataset in a zip file, you can specify the path to it below. This will extract the dataset into the dataset directory specified in step 2. It supports downloading the zip from **HuggingFace**. To get the correct link you only need to follow the steps [for models/VAEs](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#from-huggingface) but applying them to the zip file.\n","\n","zip_path = \"/content/drive/MyDrive/dataset.zip\" # @param {type: \"string\"}\n","# @markdown Specify the name of your dataset directory. If it doesn't exist, it will be created. If you have multiple dataset directories, extract each zip file into its respective dataset directory.\n","extract_to_dataset_dir = \"dataset\" # @param {type: \"string\"}\n","# @markdown Provide a [HuggingFace Access Token](https://huggingface.co/settings/tokens) if your dataset is in a private repository.\n","hf_token = \"\" # @param {type: \"string\"}\n","\n","if not \"installed_dependencies\" in globals():\n","  print(\"Installing missing dependency...\")\n","  !apt -y update -qq\n","  !apt install -y aria2 -qq\n","  globals().setdefault(\"installed_dependencies\", True)\n","\n","def extract_dataset():\n","  global zip_path\n","  is_from_hf = False\n","\n","  if not globals().get(\"second_step_done\"):\n","    print(\"You didn't complete the second step!\")\n","    return\n","\n","  if zip_path.startswith(\"https://huggingface.co/\"):\n","    is_from_hf = True\n","\n","  if not Path(zip_path).exists() and not is_from_hf:\n","    print(\"The path of the zip doesn't exists!\")\n","    return\n","\n","  if \"drive/MyDrive\" in zip_path and not Path(drive_dir).exists():\n","    print(\"Your trying to access drive but you didn't mount it!\")\n","    return\n","\n","  dataset_dir = root_path.joinpath(project_path, extract_to_dataset_dir)\n","  if Path(drive_dir).exists():\n","    dataset_dir = drive_dir.joinpath(project_path, extract_to_dataset_dir)\n","\n","  if not Path(dataset_dir).exists():\n","    Path(dataset_dir).mkdir(exist_ok=True)\n","    print(f\"Created dataset directory on new location because it didn't exist before: {dataset_dir}\")\n","\n","  if is_from_hf and re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\\.zip\", zip_path):\n","    print(\"Zip file from HuggingFace detected, attempting to download...\")\n","\n","    if \"blob\" in zip_path:\n","      zip_path = zip_path.replace(\"blob\", \"resolve\")\n","    header = f\"Authorization: Bearer {hf_token}\" if hf_token else \"\"\n","\n","    !aria2c \"{zip_path}\" --console-log-level=warn --header=\"{header}\" -c -s 16 -x 16 -k 10M -d / -o \"/content/dataset.zip\"\n","    zip_path = \"/content/dataset.zip\"\n","  elif is_from_hf and not re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\\.zip\", zip_path):\n","    print(\"Invalid URL provided for downloading the zip file.\")\n","    return\n","\n","  print(\"Extracting dataset...\")\n","\n","  with zipfile.ZipFile(zip_path, 'r') as f:\n","    f.extractall(dataset_dir)\n","\n","  print(f\"Dataset extracted in {dataset_dir}\")\n","\n","  if is_from_hf:\n","    print(\"Removing temporary zip file...\")\n","    !rm \"{zip_path}\"\n","    print(\"Done!\")\n","\n","extract_dataset()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"J86M4s3ohUYv"},"outputs":[],"source":["# @markdown ### Tag your images\n","import os\n","from pathlib import Path\n","\n","# @markdown As the name suggests, this is the type of tagging you want for your dataset.\n","method = \"Anime\" # @param [\"Anime\", \"Photorealistic\"]\n","# @markdown `(Only applies to Anime method)` The default model used for tagging is `SmilingWolf/wd-swinv2-tagger-v3`. I find it more accurate than its v2 version, but if you have experience, you can use another one and tweak the parameters. If you don't, the default configuration should be fine.\n","model = \"SmilingWolf/wd-swinv2-tagger-v3\" # @param [\"SmilingWolf/wd-swinv2-tagger-v3\", \"SmilingWolf/wd-vit-tagger-v3\", \"SmilingWolf/wd-convnext-tagger-v3\", \"SmilingWolf/wd-v1-4-swinv2-tagger-v2\", \"SmilingWolf/wd-v1-4-moat-tagger-v2\", \"SmilingWolf/wd-v1-4-convnextv2-tagger-v2\", \"SmilingWolf/wd-v1-4-convnext-tagger-v2\", \"SmilingWolf/wd-v1-4-vit-tagger-v2\"]\n","# @markdown The directory name of the dataset you want to tag. You can specify another directory when the previous one is fully tagged, in case you have more than one dataset.\n","dataset_dir_name = \"dataset\" # @param {type: \"string\"}\n","# @markdown The type of file to save your captions.\n","file_extension = \".txt\" # @param [\".txt\", \".caption\"]\n","# @markdown `(Only applies to Anime method)` Specify the tags that you don't want the autotagger to use. Separate each one with a comma `(,)` like this: **1girl, solo, standing, ...**\n","blacklisted_tags = \"\" # @param {type: \"string\"}\n","# @markdown `(Only applies to Anime method)` Specify the minimum confidence level required for assigning a tag to the image. A lower threshold results in more tags being assigned. The recommended default value for v2 taggers is 0.35 and for v3 is 0.25.\n","threshold = 0.25 # @param {type: \"slider\", min:0.0, max: 1.0, step:0.01}\n","# @markdown `(Only applies to Photorealistic method)` Specify the minimum number of words (also known as tokens) to include in the captions.\n","caption_min = 10 # @param {type: \"number\"}\n","# @markdown `(Only applies to Photorealistic method)` Specify the maximum number of words (also known as tokens) to include in the captions.\n","caption_max = 75 # @param {type: \"number\"}\n","\n","blacklisted_tags = blacklisted_tags.replace(\" \", \"\")\n","\n","def caption_images():\n","  global use_onnx_runtime\n","\n","  if not globals().get(\"second_step_done\"):\n","    print(\"You didn't complete the second step!\")\n","    return\n","\n","  dataset_dir = root_path.joinpath(project_path, dataset_dir_name)\n","  if Path(drive_dir).exists():\n","    dataset_dir = drive_dir.joinpath(project_path, dataset_dir_name)\n","\n","  sd_scripts = trainer_dir.joinpath(\"sd_scripts\")\n","  if not globals().get(\"first_step_done\"):\n","    print(\"Please run the step 1 first.\")\n","    return\n","\n","  if not globals().get(\"tagger_dependencies\"):\n","    print(\"Installing missing dependencies...\")\n","    !{venv_pip} install fairscale==0.4.13 timm==0.6.12\n","    !{venv_pip} install onnxruntime-gpu==1.17.1 --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n","    globals().setdefault(\"tagger_dependencies\", True)\n","\n","  batch_size = 8 if \"v3\" in model or \"swinv2\" in model else 1\n","\n","  model_dir = tagger_models_dir.joinpath(model.split(\"/\")[-1])\n","\n","  print(\"Tagging images\")\n","\n","  if method == \"Anime\":\n","    !{venv_python} {wd_path} \\\n","      {dataset_dir} \\\n","      --repo_id={model} \\\n","      --model_dir={model_dir} \\\n","      --thresh={threshold} \\\n","      --batch_size={batch_size} \\\n","      --max_data_loader_n_workers=2 \\\n","      --caption_extension={file_extension} \\\n","      --undesired_tags={blacklisted_tags} \\\n","      --remove_underscore \\\n","      --onnx\n","  else:\n","    os.chdir(sd_scripts)\n","    !{venv_python} finetune/make_captions.py \\\n","      {dataset_dir} \\\n","      --beam_search \\\n","      --max_data_loader_n_workers=2 \\\n","      --batch_size=8 \\\n","      --min_length={caption_min} \\\n","      --max_length={caption_max} \\\n","      --caption_extension=.txt\n","    os.chdir(root_path)\n","\n","  print(\"Tagging complete!\")\n","\n","caption_images()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"PC5JsouHTr26"},"outputs":[],"source":["# @title ## 5. Start the training\n","from pathlib import Path\n","\n","# @markdown Execute this cell to obtain the paths. Input these paths into the UI to start the training.\n","\n","def print_paths():\n","  if not globals().get(\"second_step_done\"):\n","    print(\"You didn't complete the second step!\")\n","    return\n","\n","  dataset_dirs = []\n","  project_base_dir = root_path.joinpath(project_path)\n","  if globals().get(\"use_drive\"):\n","    project_base_dir = drive_dir.joinpath(project_path)\n","\n","  for id, p_dataset_m_dir in enumerate(dataset_dir_name.replace(\" \", \"\").split(',')):\n","    dataset_dirs.append(f\"Dataset directory {id + 1}: {project_base_dir.joinpath(p_dataset_m_dir)}\")\n","\n","  model_path = model_file or \"None or you didn't run the cell to download it either because you forgot or because you have the model in drive\"\n","  vae_path = vae_file or \"None or you didn't run the cell to download it either because you forgot or because you have the VAE in drive\"\n","  output_path = project_base_dir.joinpath(output_dir_name)\n","\n","  print(\"Dataset paths:\\n  {0}\\nModel path: {1}\\nVAE path: {2}\\nOutput path: {3}\\nConfig file path: {4}\\nTags file path: {4}\".format('\\n  '.join(dataset_dirs), model_path.as_posix().replace(\" \", \"\"), vae_path, output_path, \"It's saved locally on your machine\"))\n","\n","print_paths()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"80gDArpjBLk-"},"outputs":[],"source":["import os\n","import json\n","from pathlib import Path\n","\n","# @markdown Running this cell will create a tunnel that allows you to connect from your local UI so you can send the training settings to colab. If you don't have it installed, please install it [here](https://github.com/derrian-distro/LoRA_Easy_Training_Scripts). If you are using the forked version of the trainer, you can install it [here](https://github.com/Jelosus2/LoRA_Easy_Training_Colab_Frontend). [Instructions for installation](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-install-the-ui). Once you launch the UI, set up your training parameters, copy the given URL into your interface, and click \"Start training\".\n","\n","# @markdown `(Optional)` Ngrok is an alternative method, and you need a token that you can obtain from [Ngrok's dashboard](https://dashboard.ngrok.com/get-started/your-authtoken). I recommend using it only if you want, have experience, or if the default tunnel provider is down. [How to obtain Ngrok token](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-obtain-the-ngrok-token)\n","\n","use_ngrok = False # @param {type: \"boolean\"}\n","ngrok_token = \"\" # @param {type: \"string\"}\n","\n","fifth_step_done = False\n","\n","def init_tunnel():\n","  global fifth_step_done\n","\n","  if not globals().get(\"first_step_done\"):\n","    print(\"Please run the 1st step first.\")\n","    return\n","\n","  if not globals().get(\"second_step_done\"):\n","    print(\"You didn't complete the second step!\")\n","    return\n","\n","  config_file = trainer_dir.joinpath(\"config.json\")\n","\n","  if use_ngrok:\n","    if not ngrok_token:\n","      print(\"The ngrok token must not be empty!\")\n","      return\n","\n","    with open(config_file, 'r') as config:\n","      data = json.load(config)\n","\n","    data[\"remote_mode\"] = \"ngrok\"\n","    data[\"ngrok_token\"] = ngrok_token\n","\n","    with open(config_file, 'w') as config:\n","      json.dump(data, config, indent=2)\n","  else:\n","    with open(config_file, 'r') as config:\n","      data = json.load(config)\n","\n","    if data[\"remote_mode\"] == \"ngrok\":\n","      data[\"remote_mode\"] = \"cloudflared\"\n","      data[\"ngrok_token\"] = \"\"\n","\n","      with open(config_file, 'w') as config:\n","        json.dump(data, config, indent=2)\n","\n","  os.chdir(trainer_dir)\n","  !chmod 755 run.sh\n","  !./run.sh\n","  os.chdir(root_path)\n","\n","  fifth_step_done = True\n","\n","init_tunnel()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ufU4_DUl2Rzv"},"outputs":[],"source":["import os\n","from pathlib import Path\n","\n","# @markdown Run this cell to start the training\n","\n","# @markdown Are you training on sdxl?\n","sdxl = True # @param {type: \"boolean\"}\n","\n","def start_training(is_sdxl: bool):\n","  if not globals().get(\"fifth_step_done\"):\n","    print(\"Run the cell above this one first!\")\n","    return\n","\n","  os.chdir(trainer_dir)\n","\n","  config = Path(\"runtime_store/config.toml\").resolve()\n","  dataset = Path(\"runtime_store/dataset.toml\").resolve()\n","\n","  if not Path(config).exists() and not Path(dataset).exists():\n","    print(\"The required files were not generated while running the above cell, please check again!\")\n","    return\n","\n","  sd_scripts = Path(\"sd_scripts\").resolve()\n","  training_network = \"sdxl_train_network.py\" if is_sdxl else \"train_network.py\"\n","\n","  !{venv_python} {sd_scripts.joinpath(training_network)} \\\n","    --config_file={config} \\\n","    --dataset_config={dataset}\n","\n","  os.chdir(root_path)\n","\n","start_training(sdxl)"]},{"cell_type":"code","source":["# @title 6. Utils\n","import os\n","from pathlib import Path\n","\n","# @markdown ### LoRA Resizer\n","\n","# @markdown The path pointing to the LoRA file you want to resize.\n","lora = \"\" # @param {type: \"string\"}\n","# @markdown `(Optional)` The path of the directory where the resized LoRA will be saved. If not specified the parent directory of the loaded LoRA will be used.\n","output_dir = \"\" # @param {type: \"string\"}\n","# @markdown `(Optional)` The name for the resized LoRA file. If not specified the name of the loaded LoRA will be used appending **_resized** to it.\n","output_name = \"\" # @param {type: \"string\"}\n","# @markdown The precision for saving the resized LoRA. `fp16` is the usual precision to use. **Don't touch unless you know what you are doing!**\n","save_precision = \"fp16\" # @param [\"fp16\", \"bf16\", \"float\"]\n","# @markdown The new dimensions, aka dim, for the LoRA.\n","new_dim = 4 # @param {type: \"number\"}\n","# @markdown `(LoCon-like networks only)` The new conv dimensions, aka conv dim, for the LoRA. Only use on networks that are trained with conv. For example: **LoCon, LyCORIS, LoHa, Lokr, etc**. Keep the value less than 1 to omit it's usage.\n","new_conv_dim = 0 # @param {type: \"number\"}\n","# @markdown Enables/disables the usage of `dynamic_method` and `dynamic_param`. **Don't touch unless you know what you are doing!**\n","use_dynamic = False # @param {type: \"boolean\"}\n","# @markdown Method used to calculate the resize. `sv_fro` is the usual method to use.\n","dynamic_method = \"sv_fro\" # @param [\"sv_fro\", \"sv_ratio\", \"sv_cumulative\"]\n","# @markdown Value used by the `dynamic_method` to calculate the resize.\n","dynamic_param = 0.9700 # @param {type: \"number\"}\n","# @markdown Use the GPU resources to resize the LoRA. If disabled it will use the CPU which is **not recommended!**\n","use_gpu = True # @param {type: \"boolean\"}\n","# @markdown Prints in the console the information about the resizing when the process finishes.\n","verbose_printing = False # @param {type: \"boolean\"}\n","# @markdown `(LoCon-like networks only)` Removes the conv dim layers from the LoRA. Only use on networks that are trained with conv. For example: **LoCon, LyCORIS, LoHa, Lokr, etc. Don't touch unless you know what you are doing!**\n","remove_conv_dims = False # @param {type: \"boolean\"}\n","# @markdown Removes the linear dim layers (which is what is trained usually in a LoRA) from the LoRA. **Don't touch unless you know what you are doing!**\n","remove_linear_dims = False # @param {type: \"boolean\"}\n","\n","def validate() -> tuple[bool, bool]:\n","  global output_dir, output_name\n","\n","  failed = False\n","  use_conv = True\n","  if not globals().get(\"first_step_done\"):\n","    print(\"Please run the 1st step first.\")\n","    failed = True\n","\n","  if not Path(lora).is_file() or Path(lora).suffix not in [\".ckpt\", \".safetensors\"]:\n","    print(\"The path to the LoRA file is invalid.\")\n","    failed = True\n","\n","  if not Path(output_dir).is_dir() or not output_dir:\n","    output_dir = Path(output_dir).parent if output_dir else Path(lora).parent\n","    if not output_dir.is_dir():\n","      print(\"The path to the output folder is invalid, or not a folder\")\n","      failed = True\n","    output_dir = output_dir.as_posix()\n","\n","  if not output_name:\n","    output_name = f\"{Path(lora).name.split('.')[0]}_resized\"\n","  else:\n","    output_name = output_name.split(\".\")[0]\n","\n","  if Path(output_dir).joinpath(f\"{output_name}.safetensors\").exists():\n","    idx = 1\n","    temp_name = output_name\n","    while Path(output_dir).joinpath(f\"{output_name}.safetensors\").exists():\n","      output_name = f\"{temp_name}_{idx}\"\n","      idx += 1\n","\n","    print(f\"Duplicated file in the output directory, file name changed to {output_name}\")\n","\n","  if new_dim < 1:\n","    print(\"The new dim must be 1 or greater\")\n","    failed = True\n","\n","  if new_conv_dim < 1:\n","    print(\"Skipping setting new conv dim, using new dim only\")\n","    use_conv = False\n","\n","  if use_dynamic and dynamic_param <= 0:\n","    print(\"The dynamic param must be greater than 0\")\n","    failed = True\n","\n","  return failed, use_conv\n","\n","def resize_lora(use_conv: bool):\n","  output_file = Path(output_dir).joinpath(f\"{output_name}.safetensors\").resolve()\n","\n","  new_conv_arg = f\"--new_conv_rank={new_conv_dim}\" if use_conv else \"\"\n","  dynamic_method_arg = f\"--dynamic_method={dynamic_method}\" if use_dynamic else \"\"\n","  dynamic_param_arg = \"--dynamic_param={0:.4f}\".format(dynamic_param) if use_dynamic else \"\"\n","\n","  os.chdir(trainer_dir)\n","\n","  !{venv_python} {Path(\"utils/resize_lora.py\").resolve()} \\\n","    --model={lora} \\\n","    --save_precision={save_precision} \\\n","    --new_rank={new_dim} \\\n","    --save_to={output_file} \\\n","    {new_conv_arg} \\\n","    {dynamic_method_arg} \\\n","    {dynamic_param_arg} \\\n","    {\"--verbose\" if verbose_printing else \"\"} \\\n","    {\"--device=cuda\" if use_gpu else \"\"} \\\n","    {\"--del_conv\" if remove_conv_dims else \"\"} \\\n","    {\"--del_linear\" if remove_linear_dims else \"\"} \\\n","\n","  os.chdir(root_path)\n","\n","def main():\n","  failed, use_conv = validate()\n","  if failed:\n","    return\n","\n","  resize_lora(use_conv)\n","\n","main()"],"metadata":{"id":"pEf-buIXyDLg","cellView":"form"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}