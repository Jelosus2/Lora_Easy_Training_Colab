{"cells":[{"cell_type":"markdown","metadata":{"id":"vGwaJ0eGHCkw"},"source":["# Lora Trainer XL"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"CSz_rmldHZvh"},"outputs":[],"source":["# @title ## 1. Install the trainer\n","import os\n","\n","root_path = \"/content\"\n","trainer_dir = os.path.join(root_path, \"trainer\")\n","\n","# @markdown Select the version of the trainer you want to use\n","trainer_version = \"Original\" # @param [\"Original\", \"Forked (Added CAME and REX)\"]\n","\n","installed_dependencies = False\n","first_step_done = False\n","\n","try:\n","  print(\"Installing trainer...\")\n","  !apt -y update -qq\n","  !apt install -y python3.10-venv aria2 -qq\n","\n","  installed_dependencies = True\n","\n","  if trainer_version == \"Original\":\n","    !git clone https://github.com/derrian-distro/LoRA_Easy_Training_scripts_Backend {trainer_dir}\n","  else:\n","    !git clone https://github.com/Jelosus2/LoRA_Easy_Training_scripts_Backend {trainer_dir}\n","  !chmod 755 /content/trainer/install_310.sh\n","\n","  os.chdir(trainer_dir)\n","  !yes | ./install_310.sh\n","\n","  if \"Forked\" in trainer_version:\n","    print(\"Patching trainer...\")\n","    !rm ./sd_scripts/library/train_util.py\n","    !cp ./custom/train_util.py ./sd_scripts/library/train_util.py\n","\n","  os.chdir(root_path)\n","  first_step_done = True\n","  print(\"Done!\")\n","except:\n","  print(\"Error intalling the trainer!\")\n","  first_step_done = False"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"oS4dJqXoiyC5"},"outputs":[],"source":["# @title ## 2. Setup the directories\n","import os\n","import shutil\n","from google.colab import drive\n","\n","root_path = \"/content\"\n","trainer_dir = os.path.join(root_path, \"trainer\")\n","drive_dir = os.path.join(root_path, \"drive/MyDrive\")\n","pretrained_model_dir = os.path.join(root_path, \"pretrained_model\")\n","vae_dir = os.path.join(root_path, \"vae\")\n","tagger_models_dir = os.path.join(root_path, \"tagger_models\")\n","\n","# @markdown The name for your project. Make sure it can be used as a folder name\n","project_name = \"My_first_lora\" # @param {type: \"string\"}\n","# @markdown Specify the name for the directories. If you use drive it will be created on /content/drive/MyDrive. If you have multiple datasets separate each with a (,) (dataset1, dataset2, ...)\n","output_dir_name = \"output\" # @param {type: \"string\"}\n","dataset_dir_name = \"dataset\" # @param {type: \"string\"}\n","config_file_dir_name = \"config\" # @param {type: \"string\"}\n","tags_file_dir_name = \"tags\" # @param {type: \"string\"}\n","# @markdown Use Drive to store all the files and directories\n","use_drive = True # @param {type: \"boolean\"}\n","\n","second_step_done = False\n","\n","config_dir = os.path.join(root_path, project_name, config_file_dir_name)\n","tags_dir = os.path.join(root_path, project_name, tags_file_dir_name)\n","\n","if use_drive:\n","  config_dir = os.path.join(drive_dir, project_name, config_file_dir_name)\n","  tags_dir = os.path.join(drive_dir, project_name, tags_file_dir_name)\n","\n","def mount_drive_dir():\n","  base_dir = os.path.join(root_path, project_name)\n","\n","  if use_drive:\n","    if not os.path.exists(drive_dir):\n","      drive.mount(os.path.dirname(drive_dir))\n","    base_dir = os.path.join(drive_dir, project_name)\n","\n","  return base_dir\n","\n","def make_directories():\n","  mount_drive = mount_drive_dir()\n","  output_dir = os.path.join(mount_drive, output_dir_name)\n","\n","  if use_drive and os.path.exists(os.path.join(root_path, project_name)):\n","    shutil.rmtree(os.path.join(root_path, project_name))\n","  elif not use_drive and os.path.exists(os.path.join(drive_dir, project_name)):\n","    shutil.rmtree(os.path.join(drive_dir, project_name))\n","\n","  for dir in [pretrained_model_dir, vae_dir, output_dir, tagger_models_dir, config_dir, tags_dir]:\n","    os.makedirs(dir, exist_ok=True)\n","\n","  for dataset_m_dir in dataset_dir_name.replace(\" \", \"\").split(','):\n","    os.makedirs(os.path.join(mount_drive, dataset_m_dir), exist_ok=True)\n","\n","def main():\n","  print(\"Setting up directories...\")\n","  make_directories()\n","  print(\"Done!\")\n","\n","try:\n","  main()\n","  second_step_done = True\n","except:\n","  print(\"Error setting up the directories!\")\n","  second_step_done = False"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"b0_HNDa7Zdei"},"outputs":[],"source":["# @title ## 3. Download the base model and/or VAE used for training\n","\n","model_url = \"\"\n","vae_url = \"\"\n","model_name = \"\"\n","\n","# @markdown Default models provided here for training, if you want to use another one introduce the URL in the input below.\n","training_model = \"(XL) PonyDiffusion v6\" # @param [\"(XL) PonyDiffusion v6\", \"(XL) Animagine\", \"(XL) SDXL 1.0\", \"(1.5) anime-full-final-pruned (Most used on Anime LoRas)\", \"(1.5) AnyLora\", \"(1.5) SD 1.5\"]\n","custom_training_model = \"\" # @param {type: \"string\"}\n","# @markdown VAE for training. Not needed for 1.5 nor XL but it's recommended to use SDXL base VAE on XL training. if you want to use a custom one introduce the URL in the input below.\n","vae = \"SDXL VAE\" # @param [\"SDXL VAE\", \"None\"]\n","custom_vae = \"\" # @param {type: \"string\"}\n","\n","if custom_training_model:\n","  model_url = custom_training_model\n","elif \"Pony\" in training_model:\n","  model_url = \"https://civitai.com/api/download/models/290640\"\n","  model_name = \"ponydiffusion_v6.safetensors\"\n","elif \"Animagine\" in training_model:\n","  model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-3.0/resolve/main/animagine-xl-3.0.safetensors\"\n","elif \"SDXL\" in training_model:\n","  model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\"\n","elif \"anime\" in training_model:\n","  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/animefull-final-pruned-fp16.safetensors\"\n","elif \"Any\" in training_model:\n","  model_url = \"https://huggingface.co/Lykon/AnyLoRA/resolve/main/AnyLoRA_noVae_fp16-pruned.safetensors\"\n","elif \"SD 1.5\" in training_model:\n","  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/sd-v1-5-pruned-noema-fp16.safetensors\"\n","\n","if custom_vae:\n","  vae_url = custom_vae\n","elif \"SDXL\" in vae:\n","  vae_url = \"https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors\"\n","\n","model_file = f\"/content/pretrained_model/{model_name}\"\n","vae_file = \"\"\n","\n","if not \"installed_dependencies\" in globals():\n","  print(\"Installing missing dependency...\")\n","  !apt -y update -qq\n","  !apt install -y aria2 -qq\n","  globals().setdefault(\"installed_dependencies\", True)\n","\n","if not vae == \"None\":\n","  stripped_model_vae = vae_url.strip()\n","\n","  if stripped_model_vae.lower().endswith((\".ckpt\", \".safetensors\")):\n","    vae_file = f\"content/vae{stripped_model_vae[stripped_model_vae.rfind('/'):]}\"\n","  else:\n","    vae_file = \"/content/vae/downloaded_vae.safetensors\"\n","    if os.path.exists(vae_file):\n","      !rm \"{vae_file}\"\n","\n","stripped_model_url = model_url.strip()\n","\n","if stripped_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n","  model_file = f\"content/pretrained_model{stripped_model_url[stripped_model_url.rfind('/'):]}\"\n","elif not \"pony\" in model_file:\n","  model_file = \"/content/pretrained_model/downloaded_model.safetensors\"\n","  if os.path.exists(model_file):\n","    !rm \"{model_file}\"\n","\n","print(f\"Downloading model from {model_url}...\")\n","!aria2c \"{model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n","\n","if not vae == \"None\":\n","  print(f\"Downloading vae from {vae_url}...\")\n","  !aria2c \"{vae_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{vae_file}\"\n","\n","print(\"Models downloaded successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"66XBK6B_iSYj"},"outputs":[],"source":["# @title ## 4. Upload your dataset\n","import os\n","import zipfile\n","\n","# @markdown ### Unzip the dataset\n","# @markdown You can extract your dataset in your dataset directory.\n","\n","zip_path = \"/content/drive/MyDrive/dataset.zip\" # @param {type: \"string\"}\n","# @markdown If you have multiple dataset directories extract each zip in each dataset directory.\n","extract_to_dataset_dir = \"dataset\" # @param {type: \"string\"}\n","\n","def extract_dataset():\n","  if not globals().get(\"second_step_done\"):\n","    print(\"You didn't complete the second step!\")\n","    return\n","\n","  if not os.path.exists(zip_path):\n","    print(\"The path of the zip doesn't exists!\")\n","    return\n","\n","  if \"drive/MyDrive\" in zip_path and not os.path.exists(drive_dir):\n","    print(\"Your trying to access drive but you didn't mount it!\")\n","    return\n","\n","  dataset_dir = os.path.join(root_path, project_name, extract_to_dataset_dir)\n","  if os.path.exists(drive_dir):\n","    dataset_dir = os.path.join(drive_dir, project_name, extract_to_dataset_dir)\n","\n","  if not os.path.exists(dataset_dir):\n","    os.makedirs(dataset_dir, exist_ok=True)\n","    print(f\"Created dataset directory on new location because it didn't exist before: {dataset_dir}\")\n","\n","  print(\"Extracting dataset...\")\n","\n","  with zipfile.ZipFile(zip_path, 'r') as f:\n","    f.extractall(dataset_dir)\n","\n","  print(f\"Dataset extracted in {dataset_dir}\")\n","\n","extract_dataset()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"J86M4s3ohUYv"},"outputs":[],"source":["# @markdown ### Tag your images\n","import os\n","\n","method = \"Anime\" # @param [\"Anime\", \"Photorealistic\"]\n","model = \"SmilingWolf/wd-v1-4-swinv2-tagger-v2\" # @param [\"SmilingWolf/wd-v1-4-swinv2-tagger-v2\", \"SmilingWolf/wd-v1-4-moat-tagger-v2\", \"SmilingWolf/wd-v1-4-convnextv2-tagger-v2\", \"SmilingWolf/wd-v1-4-convnext-tagger-v2\", \"SmilingWolf/wd-v1-4-vit-tagger-v2\"]\n","dataset_dir_name = \"dataset\" # @param {type: \"string\"}\n","blacklisted_tags = \"\" # @param {type: \"string\"}\n","threshold = 0.35 # @param {type: \"slider\", min:0.0, max: 1.0, step:0.01}\n","caption_min = 10 # @param {type: \"number\"}\n","caption_max = 75 # @param {type: \"number\"}\n","\n","def caption_images():\n","  if not globals().get(\"second_step_done\"):\n","    print(\"You didn't complete the second step!\")\n","    return\n","\n","  dataset_dir = os.path.join(root_path, project_name, dataset_dir_name)\n","  if os.path.exists(drive_dir):\n","    dataset_dir = os.path.join(drive_dir, project_name, dataset_dir_name)\n","\n","  sd_scripts = os.path.join(trainer_dir, \"sd_scripts\")\n","  if not os.path.exists(sd_scripts):\n","    print(\"Please run the step 1 first.\")\n","    return\n","\n","  try:\n","    import accelerate\n","  except Exception:\n","    print(\"Installing missing dependencies...\")\n","    !pip install accelerate==0.25.0 diffusers[torch]==0.21.2 einops==0.6.0\n","\n","  try:\n","    import timm\n","  except Exception:\n","    print(\"Installing missing dependencies for BLIP Captioning...\")\n","    !pip install timm==0.6.12 fairscale==0.4.13\n","\n","  print(\"Tagging images\")\n","  %env PYTHONPATH={sd_scripts}\n","\n","  if method == \"Anime\":\n","    !python {sd_scripts}/finetune/tag_images_by_wd14_tagger.py \\\n","      {dataset_dir} \\\n","      --repo_id={model} \\\n","      --model_dir={tagger_models_dir} \\\n","      --thresh={threshold} \\\n","      --batch_size=8 \\\n","      --max_data_loader_n_workers=2 \\\n","      --caption_extension=.txt \\\n","      --undesired_tags={blacklisted_tags} \\\n","      --force_download \\\n","      --remove_underscore\n","  else:\n","    os.chdir(sd_scripts)\n","    !python finetune/make_captions.py \\\n","      {dataset_dir} \\\n","      --beam_search \\\n","      --max_data_loader_n_workers=2 \\\n","      --batch_size=8 \\\n","      --min_length={caption_min} \\\n","      --max_length={caption_max} \\\n","      --caption_extension=.txt\n","    os.chdir(root_path)\n","\n","  %env PYTHONPATH=/env/python\n","  print(\"Tagging complete!\")\n","\n","caption_images()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"PC5JsouHTr26"},"outputs":[],"source":["# @title ## 5. Start the training\n","import os\n","\n","# @markdown ###TODO: Explain it\n","\n","def init_training():\n","  if not os.path.exists(trainer_dir):\n","    print(\"Please run the 1st step first.\")\n","    return\n","\n","  if not globals().get(\"second_step_done\"):\n","    print(\"You didn't complete the second step!\")\n","    return\n","\n","  os.chdir(trainer_dir)\n","  !chmod 755 run.sh\n","  !./run.sh\n","  os.chdir(root_path)\n","\n","init_training()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
