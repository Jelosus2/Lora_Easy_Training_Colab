{"cells":[{"cell_type":"markdown","metadata":{"id":"vGwaJ0eGHCkw"},"source":["# Lora_Easy_Training_Colab\n","[![Ko-Fi](https://img.shields.io/badge/Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat)](https://ko-fi.com/jelosus1)\n","\n","### Colab powered by [Lora_Easy_Training_Scripts_Backend](https://github.com/derrian-distro/LoRA_Easy_Training_scripts_Backend/)\n","\n","<h4><font color=\"red\"><u>DISCLAIMER:</u></font> The Forked version of the trainer is not maintained by its original author, please do not open issues there if you encounter any problem. Instead submit them in the <a href=\"https://github.com/Jelosus2/LoRA_Easy_Training_scripts_Backend/\">forked repo</a>.</h4>\n","\n","\n","---\n","\n","\n","Learn how to use the colab [here](https://civitai.com/articles/4409)\n","\n","---\n","\n","Last Update: March 10, 2024. Check the [full changelog](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#changelog)\n","\n","Changes:\n","- Modified the way Forked trainer is installed due to CAME and REX being officially implemented. You have to update the UI (both original or forked) if you want to use any of those, just open a command line in the root folder of the UI and run `git pull`\n","- Added the newly released v3 taggers and modified the script to make them work."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"CSz_rmldHZvh"},"outputs":[],"source":["# @title ## 1. Install the trainer\n","import os\n","\n","root_path = \"/content\"\n","trainer_dir = os.path.join(root_path, \"trainer\")\n","\n","# @markdown Select the version of the trainer you want to use. Forked version is an unofficial version that adds some features like different optimizers, schedulers, etc. You can check the features that are available on it [here](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#additions-of-the-forked-version)\n","trainer_version = \"Original\" # @param [\"Original\", \"Forked (Additional features)\"]\n","\n","installed_dependencies = False\n","first_step_done = False\n","\n","try:\n","  print(\"Installing trainer...\")\n","  !apt -y update -qq\n","  !apt install -y python3.10-venv aria2 -qq\n","\n","  installed_dependencies = True\n","\n","  if trainer_version == \"Original\":\n","    !git clone https://github.com/derrian-distro/LoRA_Easy_Training_scripts_Backend {trainer_dir}\n","  else:\n","    !git clone https://github.com/Jelosus2/LoRA_Easy_Training_scripts_Backend {trainer_dir}\n","\n","  !chmod 755 /content/trainer/colab_install.sh\n","  os.chdir(trainer_dir)\n","  !./colab_install.sh\n","\n","  if \"Forked\" in trainer_version:\n","    # No use right now\n","    print(\"Patching trainer...\")\n","\n","  os.chdir(root_path)\n","  first_step_done = True\n","  print(\"Done!\")\n","except Exception as e:\n","  print(f\"Error intalling the trainer!\\n{e}\")\n","  first_step_done = False"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"oS4dJqXoiyC5"},"outputs":[],"source":["# @title ## 2. Setup the directories\n","import os\n","import shutil\n","from google.colab import drive\n","\n","root_path = \"/content\"\n","trainer_dir = os.path.join(root_path, \"trainer\")\n","drive_dir = os.path.join(root_path, \"drive/MyDrive\")\n","pretrained_model_dir = os.path.join(root_path, \"pretrained_model\")\n","vae_dir = os.path.join(root_path, \"vae\")\n","tagger_models_dir = os.path.join(root_path, \"tagger_models\")\n","\n","# @markdown The name for your project. Make sure it can be used as a folder name\n","project_name = \"My_first_lora\" # @param {type: \"string\"}\n","# @markdown Specify the name for the directories. If you use Drive, it will be created at the base path of your drive. If you have multiple datasets, separate each with a comma `(,)` like this: **dataset1, dataset2, ...**\n","\n","# @markdown The directory where the results of the training will be stored.\n","output_dir_name = \"output\" # @param {type: \"string\"}\n","# @markdown The directory where your dataset(s) will be located.\n","dataset_dir_name = \"dataset\" # @param {type: \"string\"}\n","# @markdown Use Drive to store all the files and directories\n","use_drive = True # @param {type: \"boolean\"}\n","\n","project_name = project_name.replace(\" \", \"_\")\n","output_dir_name = output_dir_name.replace(\" \", \"_\")\n","\n","second_step_done = False\n","\n","def is_valid_folder_name(folder_name: str) -> bool:\n","  invalid_characters = '<>:\"/\\|?*'\n","\n","  if any(char in invalid_characters for char in folder_name):\n","    return False\n","\n","  return True\n","\n","def mount_drive_dir() -> str:\n","  base_dir = os.path.join(root_path, project_name)\n","\n","  if use_drive:\n","    if not os.path.exists(drive_dir):\n","      drive.mount(os.path.dirname(drive_dir))\n","    base_dir = os.path.join(drive_dir, project_name)\n","\n","  return base_dir\n","\n","def make_directories():\n","  mount_drive = mount_drive_dir()\n","  output_dir = os.path.join(mount_drive, output_dir_name)\n","\n","  if use_drive and os.path.exists(os.path.join(root_path, project_name)):\n","    shutil.rmtree(os.path.join(root_path, project_name))\n","  elif not use_drive and os.path.exists(os.path.join(drive_dir, project_name)):\n","    shutil.rmtree(os.path.join(drive_dir, project_name))\n","\n","  for dir in [pretrained_model_dir, vae_dir, output_dir, tagger_models_dir]:\n","    os.makedirs(dir, exist_ok=True)\n","\n","  for dataset_m_dir in dataset_dir_name.replace(\" \", \"\").split(','):\n","    if is_valid_folder_name(dataset_m_dir):\n","      os.makedirs(os.path.join(mount_drive, dataset_m_dir), exist_ok=True)\n","    else:\n","      print(f\"{dataset_m_dir} is not a valid name for a folder\")\n","      return\n","\n","def main():\n","  for name in [project_name, output_dir_name]:\n","      if not is_valid_folder_name(name):\n","        print(f\"{name} is not a valid name for a folder\")\n","        return\n","\n","  print(\"Setting up directories...\")\n","  make_directories()\n","  print(\"Done!\")\n","\n","try:\n","  main()\n","  second_step_done = True\n","except Exception as e:\n","  print(f\"Error setting up the directories!\\n{e}\")\n","  second_step_done = False"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"b0_HNDa7Zdei"},"outputs":[],"source":["# @title ## 3. Download the base model and/or VAE used for training\n","import os\n","import re\n","\n","model_url = \"\"\n","vae_url = \"\"\n","model_name = \"\"\n","\n","# @markdown Default models are provided here for training. If you want to use another one, introduce the URL in the input below. The link must be pointing to either Civitai or Hugging Face and have the correct format. You can check how to get the correct link [here](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-get-the-link-for-custom-modelvae).\n","training_model = \"(XL) PonyDiffusion v6\" # @param [\"(XL) PonyDiffusion v6\", \"(XL) Animagine\", \"(XL) SDXL 1.0\", \"(1.5) anime-full-final-pruned (Most used on Anime LoRas)\", \"(1.5) AnyLora\", \"(1.5) SD 1.5\"]\n","custom_training_model = \"\" # @param {type: \"string\"}\n","# @markdown VAE used for training. It's not needed for 1.5 nor XL, but it's recommended to use the SDXL base VAE for XL training. If you want to use a custom one, introduce the URL in the input below.\n","vae = \"SDXL VAE\" # @param [\"SDXL VAE\", \"None\"]\n","custom_vae = \"\" # @param {type: \"string\"}\n","\n","thrid_step_done = False\n","\n","if custom_training_model:\n","  model_url = custom_training_model\n","elif \"Pony\" in training_model:\n","  model_url = \"https://civitai.com/api/download/models/290640\"\n","  model_name = \"ponydiffusion_v6.safetensors\"\n","elif \"Animagine\" in training_model:\n","  model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-3.0/resolve/main/animagine-xl-3.0.safetensors\"\n","elif \"SDXL\" in training_model:\n","  model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\"\n","elif \"anime\" in training_model:\n","  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/animefull-final-pruned-fp16.safetensors\"\n","elif \"Any\" in training_model:\n","  model_url = \"https://huggingface.co/Lykon/AnyLoRA/resolve/main/AnyLoRA_noVae_fp16-pruned.safetensors\"\n","elif \"SD 1.5\" in training_model:\n","  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/sd-v1-5-pruned-noema-fp16.safetensors\"\n","\n","if custom_vae:\n","  vae_url = custom_vae\n","elif \"SDXL\" in vae:\n","  vae_url = \"https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors\"\n","\n","model_file = \"\"\n","vae_file = \"\"\n","\n","if not \"installed_dependencies\" in globals():\n","  print(\"Installing missing dependency...\")\n","  !apt -y update -qq\n","  !apt install -y aria2 -qq\n","  globals().setdefault(\"installed_dependencies\", True)\n","\n","def download_model(model_url, model_name):\n","  global model_file\n","\n","  if re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", model_url):\n","    model_url = model_url.replace(\"blob\", \"resolve\")\n","  elif re.search(r\"https:\\/\\/civitai\\.com\\/models\\/\\d+\", model_url):\n","    if m := re.search(r\"modelVersionId=(\\d+)\", model_url):\n","      model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n","  elif not re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", model_url) and not re.search(r\"https:\\/\\/civitai\\.com\\/api\\/download\\/models\\/(\\d+)\", model_url):\n","    print(\"Invalid model download URL!\\nCheck how to get the correct link in https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-get-the-link-for-custom-modelvae\")\n","    return\n","\n","  stripped_model_url = model_url.strip()\n","\n","  if model_name:\n","    model_file = f\"/content/pretrained_model/{model_name}\"\n","  elif stripped_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n","    model_file = f\"/content/pretrained_model{stripped_model_url[stripped_model_url.rfind('/'):]}\"\n","  elif not \"pony\" in model_file:\n","    model_file = \"/content/pretrained_model/downloaded_model.safetensors\"\n","    if os.path.exists(model_file):\n","      !rm \"{model_file}\"\n","\n","  print(f\"Downloading model from {model_url}...\")\n","  !aria2c \"{model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n","\n","def download_vae(vae_url):\n","  global vae_file\n","\n","  if not vae == \"None\":\n","    if re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", vae_url):\n","      vae_url = vae_url.replace(\"blob\", \"resolve\")\n","    elif re.search(r\"https:\\/\\/civitai\\.com\\/models\\/\\d+\", vae_url):\n","      if m := re.search(r\"modelVersionId=(\\d+)\", vae_url):\n","        vae_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n","    elif not re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", vae_url) and not re.search(r\"https:\\/\\/civitai\\.com\\/api\\/download\\/models\\/(\\d+)\", vae_url):\n","      print(\"Invalid VAE download URL!\\nCheck how to get the correct link in https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-get-the-link-for-custom-modelvae\")\n","      return\n","\n","    stripped_model_vae = vae_url.strip()\n","\n","    if stripped_model_vae.lower().endswith((\".ckpt\", \".safetensors\")):\n","      vae_file = f\"/content/vae{stripped_model_vae[stripped_model_vae.rfind('/'):]}\"\n","    else:\n","      vae_file = \"/content/vae/downloaded_vae.safetensors\"\n","      if os.path.exists(vae_file):\n","        !rm \"{vae_file}\"\n","\n","    print(f\"Downloading vae from {vae_url}...\")\n","    !aria2c \"{vae_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{vae_file}\"\n","  else:\n","    vae_file = \"\"\n","\n","try:\n","  download_model(model_url, model_name)\n","  download_vae(vae_url)\n","  thrid_step_done = True\n","except Exception as e:\n","  print(f\"Failed to download the models\\n{e}\")\n","  thrid_step_done = False"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"66XBK6B_iSYj"},"outputs":[],"source":["# @title ## 4. Upload your dataset\n","import os\n","import re\n","import zipfile\n","\n","# @markdown ### Unzip the dataset\n","# @markdown If you have a dataset in a zip file, you can specify the path to it below. This will extract the dataset into the dataset directory specified in step 2. It supports downloading the zip from **HuggingFace**. To get the correct link you only need to follow the steps [for models/VAEs](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#from-huggingface) but applying them to the zip file.\n","\n","zip_path = \"/content/drive/MyDrive/dataset.zip\" # @param {type: \"string\"}\n","# @markdown Specify the name of your dataset directory. If it doesn't exist, it will be created. If you have multiple dataset directories, extract each zip file into its respective dataset directory.\n","extract_to_dataset_dir = \"dataset\" # @param {type: \"string\"}\n","\n","if not \"installed_dependencies\" in globals():\n","  print(\"Installing missing dependency...\")\n","  !apt -y update -qq\n","  !apt install -y aria2 -qq\n","  globals().setdefault(\"installed_dependencies\", True)\n","\n","def extract_dataset():\n","  global zip_path\n","  is_from_hf = False\n","\n","  if not globals().get(\"second_step_done\"):\n","    print(\"You didn't complete the second step!\")\n","    return\n","\n","  if zip_path.startswith(\"https://huggingface.co/\"):\n","    is_from_hf = True\n","\n","  if not os.path.exists(zip_path) and not is_from_hf:\n","    print(\"The path of the zip doesn't exists!\")\n","    return\n","\n","  if \"drive/MyDrive\" in zip_path and not os.path.exists(drive_dir):\n","    print(\"Your trying to access drive but you didn't mount it!\")\n","    return\n","\n","\n","  dataset_dir = os.path.join(root_path, project_name, extract_to_dataset_dir)\n","  if os.path.exists(drive_dir):\n","    dataset_dir = os.path.join(drive_dir, project_name, extract_to_dataset_dir)\n","\n","  if not os.path.exists(dataset_dir):\n","    os.makedirs(dataset_dir, exist_ok=True)\n","    print(f\"Created dataset directory on new location because it didn't exist before: {dataset_dir}\")\n","\n","  if is_from_hf and re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\\.zip\", zip_path):\n","    print(\"Zip file from HuggingFace detected, attempting to download...\")\n","    !aria2c \"{zip_path}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"/content/dataset.zip\"\n","    zip_path = \"/content/dataset.zip\"\n","  elif is_from_hf and not re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\\.zip\", zip_path):\n","    print(\"Invalid URL provided for downloading the zip file.\")\n","    return\n","\n","  print(\"Extracting dataset...\")\n","\n","  with zipfile.ZipFile(zip_path, 'r') as f:\n","    f.extractall(dataset_dir)\n","\n","  print(f\"Dataset extracted in {dataset_dir}\")\n","\n","  if is_from_hf:\n","    print(\"Removing temporary zip file...\")\n","    !rm \"{zip_path}\"\n","    print(\"Done!\")\n","\n","extract_dataset()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"J86M4s3ohUYv"},"outputs":[],"source":["# @markdown ### Tag your images\n","import os\n","\n","# @markdown As the name suggests, this is the type of tagging you want for your dataset.\n","method = \"Anime\" # @param [\"Anime\", \"Photorealistic\"]\n","# @markdown `(Only applies to Anime method)` The default model used for tagging is `SmilingWolf/wd-swinv2-tagger-v3`. I find it more accurate than its v2 version, but if you have experience, you can use another one and tweak the parameters. If you don't, the default configuration should be fine.\n","model = \"SmilingWolf/wd-swinv2-tagger-v3\" # @param [\"SmilingWolf/wd-swinv2-tagger-v3\", \"SmilingWolf/wd-vit-tagger-v3\", \"SmilingWolf/wd-convnext-tagger-v3\", \"SmilingWolf/wd-v1-4-swinv2-tagger-v2\", \"SmilingWolf/wd-v1-4-moat-tagger-v2\", \"SmilingWolf/wd-v1-4-convnextv2-tagger-v2\", \"SmilingWolf/wd-v1-4-convnext-tagger-v2\", \"SmilingWolf/wd-v1-4-vit-tagger-v2\"]\n","# @markdown The directory name of the dataset you want to tag. You can specify another directory when the previous one is fully tagged, in case you have more than one dataset.\n","dataset_dir_name = \"dataset\" # @param {type: \"string\"}\n","# @markdown `(Only applies to Anime method)` Specify the tags that you don't want the autotagger to use. Separate each one with a comma `(,)` like this: **1girl, solo, standing, ...**\n","blacklisted_tags = \"\" # @param {type: \"string\"}\n","# @markdown `(Only applies to Anime method)` Specify the minimum confidence level required for assigning a tag to the image. A lower threshold results in more tags being assigned. The recommended default value for v2 taggers is 0.35 and for v3 is 0.25.\n","threshold = 0.25 # @param {type: \"slider\", min:0.0, max: 1.0, step:0.01}\n","# @markdown `(Only applies to Photorealistic method)` Specify the minimum number of words (also known as tokens) to include in the captions.\n","caption_min = 10 # @param {type: \"number\"}\n","# @markdown `(Only applies to Photorealistic method)` Specify the maximum number of words (also known as tokens) to include in the captions.\n","caption_max = 75 # @param {type: \"number\"}\n","\n","blacklisted_tags = blacklisted_tags.replace(\" \", \"\")\n","\n","def caption_images():\n","  if not globals().get(\"second_step_done\"):\n","    print(\"You didn't complete the second step!\")\n","    return\n","\n","  dataset_dir = os.path.join(root_path, project_name, dataset_dir_name)\n","  if os.path.exists(drive_dir):\n","    dataset_dir = os.path.join(drive_dir, project_name, dataset_dir_name)\n","\n","  sd_scripts = os.path.join(trainer_dir, \"sd_scripts\")\n","  if not os.path.exists(sd_scripts):\n","    print(\"Please run the step 1 first.\")\n","    return\n","\n","  wd_path = os.path.join(sd_scripts, \"finetune\", \"tag_images_by_wd14_tagger.py\")\n","\n","  try:\n","    import accelerate\n","  except Exception:\n","    print(\"Installing missing dependencies...\")\n","    !pip install accelerate==0.25.0 diffusers[torch]==0.21.2 einops==0.6.0 onnx==1.15.0\n","    !pip install onnxruntime-gpu==1.17.1 --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n","\n","    !rm \"{wd_path}\"\n","    !aria2c \"https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/main/custom/tag_images_by_wd14_tagger.py\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{wd_path}\"\n","\n","  try:\n","    import timm\n","  except Exception:\n","    print(\"Installing missing dependencies for BLIP Captioning...\")\n","    !pip install timm==0.6.12 fairscale==0.4.13\n","\n","  use_onnx = True if \"v3\" in model else False\n","  model_dir = os.path.join(tagger_models_dir, model.split(\"/\")[-1])\n","\n","  print(\"Tagging images\")\n","  %env PYTHONPATH={sd_scripts}\n","\n","  if method == \"Anime\":\n","    !python {wd_path} \\\n","      {dataset_dir} \\\n","      --repo_id={model} \\\n","      --model_dir={model_dir} \\\n","      --thresh={threshold} \\\n","      --batch_size=8 \\\n","      --max_data_loader_n_workers=2 \\\n","      --caption_extension=.txt \\\n","      --undesired_tags={blacklisted_tags} \\\n","      --remove_underscore \\\n","      {\"--onnx\" if use_onnx else \"\"}\n","  else:\n","    os.chdir(sd_scripts)\n","    !python finetune/make_captions.py \\\n","      {dataset_dir} \\\n","      --beam_search \\\n","      --max_data_loader_n_workers=2 \\\n","      --batch_size=8 \\\n","      --min_length={caption_min} \\\n","      --max_length={caption_max} \\\n","      --caption_extension=.txt\n","    os.chdir(root_path)\n","\n","  %env PYTHONPATH=/env/python\n","  print(\"Tagging complete!\")\n","\n","caption_images()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"PC5JsouHTr26"},"outputs":[],"source":["# @title ## 5. Start the training\n","import os\n","\n","# @markdown Execute this cell to obtain the paths. Input these paths into the UI to start the training.\n","\n","def print_paths():\n","  if not globals().get(\"second_step_done\"):\n","    print(\"You didn't complete the second step!\")\n","    return\n","\n","  if not globals().get(\"thrid_step_done\"):\n","    print(\"You didn't complete the thrid step!\")\n","    return\n","\n","  dataset_dirs = []\n","  project_base_dir = os.path.join(root_path, project_name)\n","  if globals().get(\"use_drive\"):\n","    project_base_dir = os.path.join(drive_dir, project_name)\n","\n","  for id, p_dataset_m_dir in enumerate(dataset_dir_name.replace(\" \", \"\").split(',')):\n","    dataset_dirs.append(f\"Dataset directory {id + 1}: {os.path.join(project_base_dir, p_dataset_m_dir)}\")\n","\n","  model_path = model_file\n","  vae_path = vae_file or \"None\"\n","  output_path = os.path.join(project_base_dir, output_dir_name)\n","\n","  print(\"Dataset paths:\\n  {0}\\nModel path: {1}\\nVAE path: {2}\\nOutput path: {3}\\nConfig file path: {4}\\nTags file path: {4}\".format('\\n  '.join(dataset_dirs), model_path.replace(\" \", \"\"), vae_path, output_path, \"It's saved locally on your machine\"))\n","\n","print_paths()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"80gDArpjBLk-"},"outputs":[],"source":["import os\n","import json\n","\n","# @markdown Running this cell will create a tunnel that allows you to connect from your local UI. If you don't have it installed, please install it [here](https://github.com/derrian-distro/LoRA_Easy_Training_Scripts). If you are using the forked version of the trainer, you can install it [here](https://github.com/Jelosus2/LoRA_Easy_Training_Colab_Frontend). [Instructions for installation](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-install-the-ui). Once you launch the UI, set up your training parameters, copy the given URL into your interface, and click \"Start training\".\n","\n","\n","# @markdown `(Optional)` Ngrok is an alternative method, and you need a token that you can obtain from [Ngrok's dashboard](https://dashboard.ngrok.com/get-started/your-authtoken). I recommend using it only if you want, have experience, or if the default tunnel provider is down. [How to obtain Ngrok token](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-obtain-the-ngrok-token)\n","\n","use_ngrok = False # @param {type: \"boolean\"}\n","ngrok_token = \"\" # @param {type: \"string\"}\n","\n","def init_training():\n","  if not os.path.exists(trainer_dir):\n","    print(\"Please run the 1st step first.\")\n","    return\n","\n","  if not globals().get(\"second_step_done\"):\n","    print(\"You didn't complete the second step!\")\n","    return\n","\n","  config_file = os.path.join(trainer_dir, \"config.json\")\n","\n","  if use_ngrok:\n","    if not ngrok_token:\n","      print(\"The ngrok token must not be empty!\")\n","      return\n","\n","    with open(config_file, 'r') as config:\n","      data = json.load(config)\n","\n","    data[\"remote_mode\"] = \"ngrok\"\n","    data[\"ngrok_token\"] = ngrok_token\n","\n","    with open(config_file, 'w') as config:\n","      json.dump(data, config, indent=2)\n","  else:\n","    with open(config_file, 'r') as config:\n","      data = json.load(config)\n","\n","    if data[\"remote_mode\"] == \"ngrok\":\n","      data[\"remote_mode\"] = \"cloudflared\"\n","      data[\"ngrok_token\"] = \"\"\n","\n","      with open(config_file, 'w') as config:\n","        json.dump(data, config, indent=2)\n","\n","  os.chdir(trainer_dir)\n","  !chmod 755 run.sh\n","  !./run.sh\n","  os.chdir(root_path)\n","\n","\n","init_training()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
